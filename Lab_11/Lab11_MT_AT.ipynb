{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyPTffTLug7i"
      },
      "source": [
        "# **Laboratorio 11: LLM y Agentes Aut칩nomos 游뱄**\n",
        "\n",
        "MDS7202: Laboratorio de Programaci칩n Cient칤fica para Ciencia de Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pbWVyntzbvL"
      },
      "source": [
        "### **Cuerpo Docente:**\n",
        "\n",
        "- Profesores: Ignacio Meza, Sebasti치n Tinoco\n",
        "- Auxiliar: Eduardo Moya\n",
        "- Ayudantes: Nicol치s Ojeda, Melanie Pe침a, Valentina Rojas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy6ikgVYzghB"
      },
      "source": [
        "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no ser치n revisados\n",
        "\n",
        "- Nombre de alumno 1: Mart칤n Torrico\n",
        "- Nombre de alumno 2: Alejandra Toro\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMJ-owchzjFf"
      },
      "source": [
        "### **Link de repositorio de GitHub:** [Insertar Repositorio](https://github.com/MartinTorricoP/Laboratorios_MDS7202)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUuwsXrKzmkK"
      },
      "source": [
        "## **Temas a tratar**\n",
        "\n",
        "- Reinforcement Learning\n",
        "- Large Language Models\n",
        "\n",
        "## **Reglas:**\n",
        "\n",
        "- **Grupos de 2 personas**\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente ser치n respondidos por este medio.\n",
        "- Prohibidas las copias.\n",
        "- Pueden usar cualquer matrial del curso que estimen conveniente.\n",
        "\n",
        "### **Objetivos principales del laboratorio**\n",
        "\n",
        "- Resoluci칩n de problemas secuenciales usando Reinforcement Learning\n",
        "- Habilitar un Chatbot para entregar respuestas 칰tiles usando Large Language Models.\n",
        "\n",
        "El laboratorio deber치 ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al m치ximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante m치s eficientes que los iteradores nativos sobre DataFrames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hmHHQ9BuyAG"
      },
      "source": [
        "## **1. Reinforcement Learning (2.0 puntos)**\n",
        "\n",
        "En esta secci칩n van a usar m칠todos de RL para resolver dos problemas interesantes: `Blackjack` y `LunarLander`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gOcejYb6uzOO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (box2d-py)\n"
          ]
        }
      ],
      "source": [
        "!pip install -qqq gymnasium stable_baselines3\n",
        "!pip install -qqq swig\n",
        "!pip install -qqq gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBPet_Mq8dX9"
      },
      "source": [
        "### **1.1 Blackjack (1.0 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://www.recreoviral.com/wp-content/uploads/2016/08/s3.amazonaws.com-Math.gif\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "La idea de esta subsecci칩n es que puedan implementar m칠todos de RL y as칤 generar una estrategia para jugar el cl치sico juego Blackjack y de paso puedan ~~hacerse millonarios~~ aprender a resolver problemas mediante RL.\n",
        "\n",
        "Comencemos primero preparando el ambiente. El siguiente bloque de c칩digo transforma las observaciones del ambiente a `np.array`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LpZ8bBKk9ZlU"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.spaces import MultiDiscrete\n",
        "import numpy as np\n",
        "\n",
        "class FlattenObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(FlattenObservation, self).__init__(env)\n",
        "        self.observation_space = MultiDiscrete(np.array([32, 11, 2]))\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.array(observation).flatten()\n",
        "\n",
        "# Create and wrap the environment\n",
        "env = gym.make(\"Blackjack-v1\")\n",
        "env = FlattenObservation(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ6J1_-Y9nHO"
      },
      "source": [
        "#### **1.1.1 Descripci칩n de MDP (0.2 puntos)**\n",
        "\n",
        "Entregue una breve descripci칩n sobre el ambiente [Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/) y su formulaci칩n en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5i1Wt1p770x"
      },
      "source": [
        "\n",
        "El ambiente de Blackjack adjunto consiste en una partida de Blackjack, juego donde el objetivo es que la suma de tus cartas se mayor a la del dealer sin excederse de los 21 puntos. La formulaci칩n en MDP corresponde a :\n",
        "\n",
        "* Estados:\n",
        "  - Suma actual de las cartas del jugador: Valores entre el 4 al 21.\n",
        "  - Carta visible del dealer: Valores del 1 al 10 (donde 1 es un As).\n",
        "  - As utilizable: Valor 0 o 1 (siendo 1 cuando el jugador puede utilizar un As).\n",
        "\n",
        "* Acciones:\n",
        " - Quedarse con la suma actual: 0 (stick).\n",
        " - Pedir otra carta: 1 (hit).\n",
        "\n",
        "* Recompensas:\n",
        " - Ganar: +1.\n",
        " - Perder: -1.\n",
        " - Empatar: 0.\n",
        " - Ganar con blackjack natural: +1,5 (solo si natural=True).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmcX6bRC9agQ"
      },
      "source": [
        "#### **1.1.2 Generando un Baseline (0.2 puntos)**\n",
        "\n",
        "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulaci칩n 5000 veces y reporte el promedio y desviaci칩n de las recompensas. 쮺칩mo calificar칤a el performance de esta pol칤tica? 쮺칩mo podr칤a interpretar las recompensas obtenidas?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9p2PrLLR9yju",
        "outputId": "a81d36e8-3614-40df-ef05-1cd41829f012"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Promedio de las recompensas: -0.407\n",
            "Desviaci칩n est치ndar de las recompensas: 0.8922729403047029\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "# Variables para almacenar resultados\n",
        "rewards = []\n",
        "n = 5000 #numero de repeticiones\n",
        "\n",
        "# Simulaci칩n de 5000 episodios con acciones aleatorias\n",
        "for _ in range(n):\n",
        "    obs = env.reset()  # Reiniciar el ambiente\n",
        "    done = False\n",
        "    total = 0\n",
        "\n",
        "    while not done:\n",
        "        action = env.action_space.sample() # Seleccionar una acci칩n aleatoria (0 o 1)\n",
        "        obs, reward, done, _, _ = env.step(action) #Ejecutar la acci칩n\n",
        "        total += reward\n",
        "\n",
        "    rewards.append(total)\n",
        "\n",
        "# Calcular promedio y desviaci칩n est치ndar de las recompensas\n",
        "average_reward = np.mean(rewards)\n",
        "std_deviation = np.std(rewards)\n",
        "\n",
        "print(f\"Promedio de las recompensas: {average_reward}\")\n",
        "print(f\"Desviaci칩n est치ndar de las recompensas: {std_deviation}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq-VAb7ZBksa"
      },
      "source": [
        "El desempe침o de la pol칤tica aleatoria es sub칩ptimo.\n",
        "\n",
        "Esto es, ya que contamos con un promedio de recompensas negativos (-0.407), indicando m치s p칠rdidas que ganancias, sumado a una alta variabilidad con una desviaci칩n est치ndar de 0.8922, debido a la naturaleza estoc치stica del juego.\n",
        "\n",
        "Con esta pol칤tica, podemos tener un resultado base el cual podemos mejorar, mostrandonos que no es un juego donde una buena estrategia es similar a lanzar una moneda, sino que la estrategia de cu치ndo pedir o cu치ndo quedarse es importante para tener mejores resultados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEO_dY4x_SJu"
      },
      "source": [
        "#### **1.1.3 Entrenamiento de modelo (0.2 puntos)**\n",
        "\n",
        "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `Blackjack`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9JsFA1wGmnH",
        "outputId": "73d8c12a-4dfc-4022-e7df-f212f60f5f88"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import DQN\n",
        "\n",
        "# Crear el modelo DQN\n",
        "model = DQN(\"MlpPolicy\", env, verbose=1)\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.learn(total_timesteps=5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-bpdb8wZID1"
      },
      "source": [
        "#### **1.1.4 Evaluaci칩n de modelo (0.2 puntos)**\n",
        "\n",
        "Repita el ejercicio 1.1.2 pero utilizando el modelo entrenado. 쮺칩mo es el performance de su agente? 쮼s mejor o peor que el escenario baseline?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-d7d8GFf7F6",
        "outputId": "ed9c4288-5793-4899-efc5-faa497ba276e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparaci칩n de resultados:\n",
            "Pol칤tica Aleatoria \n",
            "Promedio: -0.407, Desviaci칩n: 0.8922729403047029\n",
            "Pol칤tica DQN \n",
            "Promedio: -0.1144, Desviaci칩n: 0.9521095735260726\n"
          ]
        }
      ],
      "source": [
        "# Evaluar la pol칤tica aprendida por DQN\n",
        "dqn_rewards = []\n",
        "\n",
        "for _ in range(n):\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    total = 0\n",
        "\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs, deterministic=True)  # Pol칤tica DQN\n",
        "        obs, reward, done, _, _ = env.step(action)\n",
        "        total += reward\n",
        "\n",
        "    dqn_rewards.append(total)\n",
        "\n",
        "# Calcular promedio y desviaci칩n est치ndar de la pol칤tica DQN\n",
        "average_dqn_reward = np.mean(dqn_rewards)\n",
        "std_dqn_deviation = np.std(dqn_rewards)\n",
        "\n",
        "# Comparar resultados\n",
        "print(\"Comparaci칩n de resultados:\")\n",
        "print(f\"Pol칤tica Aleatoria \\nPromedio: {average_reward}, Desviaci칩n: {std_deviation}\")\n",
        "print(f\"Pol칤tica DQN \\nPromedio: {average_dqn_reward}, Desviaci칩n: {std_dqn_deviation}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLVoKZq_Hz_e"
      },
      "source": [
        "Podemos ver que el desempe침o del agente mejora considerablemente en cuanto el promedio de sus recompensas, subiendo su promedio a -0.1144, tomando deciciones estrat칠gicas e inteligentes y con ello ganando m치s veces que tomando deciciones al azar.\n",
        "\n",
        "Sin embargo, podemos ver tambi칠n que el desempe침o del agente tiene m치s variabilidad en sus resultados con su pol칤tica, lo cual nos indica que puede ser refinado para mejorar su toma de decisiones.\n",
        "\n",
        "Con lo anterior, este escenario es mejor al aleatorio, pero ser칤a recomendable seguir mejorando su consistencia en el desempe침o.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO-EsAaPAYEm"
      },
      "source": [
        "#### **1.1.5 Estudio de acciones (0.2 puntos)**\n",
        "\n",
        "Genere una funci칩n que reciba un estado y retorne la accion del agente. Luego, use esta funci칩n para entregar la acci칩n escogida frente a los siguientes escenarios:\n",
        "\n",
        "- Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene tiene un as\n",
        "- Suma de cartas del agente es 19, dealer muestra un 3, agente tiene tiene un as\n",
        "\n",
        "쯉on coherentes sus acciones con las reglas del juego?\n",
        "\n",
        "Hint: 쮸 que clase de python pertenecen los estados? Pruebe a usar el m칠todo `.reset` para saberlo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Fh8XlGyzwtRp"
      },
      "outputs": [],
      "source": [
        "def elegir_accion(estado, model):\n",
        "    action, _ = model.predict(estado, deterministic=True)\n",
        "    return action\n",
        "\n",
        "estado1 = [6, 7, False]\n",
        "accion1 = elegir_accion(estado1, model)\n",
        "\n",
        "estado2 = [19, 3, True]\n",
        "accion2 = elegir_accion(estado2, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOkRbjLULS03",
        "outputId": "b9286ba9-0525-46fa-d4f9-4184dd304bf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acci칩n para el estado 1 (6, 7, sin as): Quedarse\n"
          ]
        }
      ],
      "source": [
        "estado_1 = [6, 7, False]\n",
        "accion_1 = elegir_accion(estado_1, model)\n",
        "\n",
        "print(f\"Acci칩n para el estado 1 (6, 7, sin as): {'Pedir carta' if accion_1 == 1 else 'Quedarse'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BeKEBC9LwiE",
        "outputId": "10525308-26c0-47c7-c5e8-6e646c2495d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acci칩n para el estado 2 (19, 3, con as): Quedarse\n"
          ]
        }
      ],
      "source": [
        "estado_2 = [19, 3, True]\n",
        "accion_2 = elegir_accion(estado_2, model)\n",
        "\n",
        "print(f\"Acci칩n para el estado 2 (19, 3, con as): {'Pedir carta' if accion_2 == 1 else 'Quedarse'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3P9QvGS0SIDw"
      },
      "source": [
        "En este caso, solo el estado 2 tiene sentido, mientras que el estado 1 no tiene sentido, por lo que ser칤a interesante poder optimizar el modelo.\n",
        "\n",
        "Esto es porque la suma en el primer caso del agente es muy baja y ser칤a normal pedir una sigiente carta (si no pedimos, pr치cticamente perdemos directamente), mientras que en el estado 2 la suma del jugador es muy alta y seguramente sobrepase el 21 (y actualmente, la carta que tiene el dealer es muy baja).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEqCTqqroh03"
      },
      "source": [
        "### **1.2 LunarLander**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.redd.it/097t6tk29zf51.jpg\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "Similar a la secci칩n 2.1, en esta secci칩n usted se encargar치 de implementar una gente de RL que pueda resolver el ambiente `LunarLander`.\n",
        "\n",
        "Comencemos preparando el ambiente:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvQUyuZ_FtZ4",
        "outputId": "b0a8ac67-86f7-441d-acc8-b2146390b8ff"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "env = gym.make(\"LunarLander-v3\", render_mode = \"rgb_array\", continuous = True) # notar el par치metro continuous = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBU4lGX3wpN6"
      },
      "source": [
        "Noten que se especifica el par치metro `continuous = True`. 쯈ue implicancias tiene esto sobre el ambiente?\n",
        "\n",
        "Adem치s, se le facilita la funci칩n `export_gif` para el ejercicio 2.2.4:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRiWpSo9yfr9",
        "outputId": "38f87a3b-8a34-4c2a-b961-e54e0c71ac07"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "import numpy as np\n",
        "\n",
        "def export_gif(model, n = 5):\n",
        "  '''\n",
        "  funci칩n que exporta a gif el comportamiento del agente en n episodios\n",
        "  '''\n",
        "  images = []\n",
        "  for episode in range(n):\n",
        "    obs = model.env.reset()\n",
        "    img = model.env.render()\n",
        "    done = False\n",
        "    while not done:\n",
        "      images.append(img)\n",
        "      action, _ = model.predict(obs)\n",
        "      obs, reward, done, info = model.env.step(action)\n",
        "      img = model.env.render(mode=\"rgb_array\")\n",
        "\n",
        "  imageio.mimsave(\"agent_performance.gif\", [np.array(img) for i, img in enumerate(images) if i % 5 == 0], fps=15) # editado para mejor rendimiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El par치metro continuous=True transforma el problema en un desaf칤o m치s complejo al permitir un control m치s detallado del lander, lo cual tambi칠n requiere m칠todos de aprendizaje m치s sofisticados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk5VJVppXh3N"
      },
      "source": [
        "#### **1.2.1 Descripci칩n de MDP (0.2 puntos)**\n",
        "\n",
        "Entregue una breve descripci칩n sobre el ambiente [LunarLander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) y su formulaci칩n en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas. 쮺omo se distinguen las acciones de este ambiente en comparaci칩n a `Blackjack`?\n",
        "\n",
        "Nota: recuerde que se especific칩 el par치metro `continuous = True`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb-u9LUE8O9a"
      },
      "source": [
        "El ambiente de LunarLander consiste en un problema de control de trayectoria de un cohete, donde el objetivo es aterrizar en una plataforma sin salir del 치rea designada ni estrellarse. La formulaci칩n en MDP corresponde a:\n",
        "\n",
        "* Estados:\n",
        "  - Coordenadas x e y del lander (posici칩n relativa al 치rea de aterrizaje).\n",
        "  - Velocidades lineales en x e y.\n",
        "  - 츼ngulo de orientaci칩n del lander.\n",
        "  - Velocidad angular.\n",
        "  - Contacto de las patas con el suelo (dos valores booleanos).\n",
        "\n",
        "* Acciones:\n",
        "  - 0: No realizar ninguna acci칩n.\n",
        "  - 1: Activar el motor lateral izquierdo.\n",
        "  - 2: Activar el motor principal.\n",
        "  - 3: Activar el motor lateral derecho.\n",
        "\n",
        "* Recompensas:\n",
        "- Proximidad al 치rea de aterrizaje: Incrementos/penalizaciones por acercarse/alejarse.\n",
        "- Velocidad y orientaci칩n:\n",
        "  - Penalizaci칩n por velocidades altas y 치ngulos inadecuados.\n",
        "- Uso de motores:\n",
        "  - Penalizaci칩n por activaciones de motores -0,03 para laterales y -0,3 para el principal.\n",
        "- Aterrizajes:\n",
        "  - Recompensa de +10 por cada pata en contacto con el suelo.\n",
        "  - Recompensa de +100 por un aterrizaje exitoso.\n",
        "  - Penalizaci칩n de -100 por estrellarse.\n",
        "  - Un episodio se considera resuelto si el agente acumula al menos 200 puntos.\n",
        "\n",
        "**Diferencia de acciones con Blackjack**:\n",
        "\n",
        "A diferencia de Blackjack, donde las acciones son discretas y binarias (pedir o quedarse), en LunarLander las acciones pueden ser discretas (selecci칩n de motores) o **continuas** (intensidad de empuje), dependiendo de la configuraci칩n del ambiente.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YChodtNQwzG2"
      },
      "source": [
        "#### **1.2.2 Generando un Baseline (0.2 puntos)**\n",
        "\n",
        "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulaci칩n 10 veces y reporte el promedio y desviaci칩n de las recompensas. 쮺칩mo calificar칤a el performance de esta pol칤tica?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bwc3A0GX7a8",
        "outputId": "a1785b3b-9157-4d1f-8e6c-ba4f91f041e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Promedio de las recompensas: -208.50902921506776\n",
            "Desviaci칩n est치ndar de las recompensas: 143.06927422727438\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "# Variables para almacenar resultados\n",
        "rewards = []\n",
        "n = 10  # n칰mero de repeticiones\n",
        "\n",
        "# Simulaci칩n de 10 episodios con acciones aleatorias\n",
        "for _ in range(n):\n",
        "    obs = env.reset()  # Reiniciar el ambiente\n",
        "    done = False\n",
        "    total = 0\n",
        "\n",
        "    while not done:\n",
        "        action = env.action_space.sample()  # Seleccionar una acci칩n aleatoria\n",
        "        obs, reward, done, _, _ = env.step(action)  # Ejecutar la acci칩n\n",
        "        total += reward\n",
        "\n",
        "    rewards.append(total)\n",
        "\n",
        "# Calcular promedio y desviaci칩n est치ndar de las recompensas\n",
        "average_reward = np.mean(rewards)\n",
        "std_deviation = np.std(rewards)\n",
        "\n",
        "print(f\"Promedio de las recompensas: {average_reward}\")\n",
        "print(f\"Desviaci칩n est치ndar de las recompensas: {std_deviation}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf5HBRLRZq3h"
      },
      "source": [
        "El rendimiento de esta pol칤tica aleatoria nuevamente es sub칩ptima como es esperado.\n",
        "\n",
        "Podemos ver que en promedio se estrella el lander 2 veces por aterrizaje, lo cual es un muy mal desempe침o para lo que queremos y que tiene una desviaci칩n est치ndar muy alta de sus recompenzas, con un valor de 143.\n",
        "\n",
        "Claramente hay espacio de mejora y lo desarrollaremos a continuaci칩n:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQrZVQflX_5f"
      },
      "source": [
        "#### **1.2.3 Entrenamiento de modelo (0.2 puntos)**\n",
        "\n",
        "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `LunarLander` **usando 10000 timesteps de entrenamiento**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_6Ia9uoF7Hs",
        "outputId": "e28152bc-ef69-4c37-d18b-8ed14cd9f564"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO\n",
        "\n",
        "# Crear el modelo PPO\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.learn(total_timesteps=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z-oIUSrlAsY"
      },
      "source": [
        "#### **1.2.4 Evaluaci칩n de modelo (0.2 puntos)**\n",
        "\n",
        "Repita el ejercicio 1.2.2 pero utilizando el modelo entrenado. 쮺칩mo es el performance de su agente? 쮼s mejor o peor que el escenario baseline?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ophyU3KrWrwl",
        "outputId": "323f76c6-d17e-4d5a-acda-e207173eadf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparaci칩n de resultados:\n",
            "Pol칤tica Aleatoria \n",
            "Promedio: -208.50902921506776, Desviaci칩n: 143.06927422727438\n",
            "Pol칤tica PPO \n",
            "Promedio: -131.04745755518053, Desviaci칩n: 82.37868052944239\n"
          ]
        }
      ],
      "source": [
        "# Evaluar la pol칤tica aprendida por PPO\n",
        "ppo_rewards = []\n",
        "\n",
        "for _ in range(n):\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    total = 0\n",
        "\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs, deterministic=True)  # Pol칤tica PPO\n",
        "        obs, reward, done, _, _ = env.step(action)\n",
        "        total += reward\n",
        "\n",
        "    ppo_rewards.append(total)\n",
        "\n",
        "# Calcular promedio y desviaci칩n est치ndar de la pol칤tica PPO\n",
        "average_ppo_reward = np.mean(ppo_rewards)\n",
        "std_ppo_deviation = np.std(ppo_rewards)\n",
        "\n",
        "# Comparar resultados\n",
        "print(\"Comparaci칩n de resultados:\")\n",
        "print(f\"Pol칤tica Aleatoria \\nPromedio: {average_reward}, Desviaci칩n: {std_deviation}\")\n",
        "print(f\"Pol칤tica PPO \\nPromedio: {average_ppo_reward}, Desviaci칩n: {std_ppo_deviation}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qf8TctocMme"
      },
      "source": [
        "El performance de la gente nuevamente mejora, pero no de manera tan sustancial como en el caso del blackjack. En este caso, podemos ver que en promedio se estrella 1 vez el lander en vez de 2, lo cual es bueno, pero no 칩ptimo. Adem치s, viendo que su desviaci칩n tiene un valor menor, sabemos que este resultado es m치s consistente en el tiempo. Con esto, tenemos un mejor modelo que el baseline, pero con espacio a mejora."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6Xw4YHT3P5d"
      },
      "source": [
        "#### **1.2.5 Optimizaci칩n de modelo (0.2 puntos)**\n",
        "\n",
        "Repita los ejercicios 1.2.3 y 1.2.4 hasta obtener un nivel de recompensas promedio mayor a 50. Para esto, puede cambiar manualmente par치metros como:\n",
        "- `total_timesteps`\n",
        "- `learning_rate`\n",
        "- `batch_size`\n",
        "\n",
        "Una vez optimizado el modelo, use la funci칩n `export_gif` para estudiar el comportamiento de su agente en la resoluci칩n del ambiente y comente sobre sus resultados.\n",
        "\n",
        "Adjunte el gif generado en su entrega (mejor a칰n si adem치s adjuntan el gif en el markdown)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aItYF6sr6F_6"
      },
      "outputs": [],
      "source": [
        "model = PPO(\"MlpPolicy\", env, verbose=1, seed = 123)\n",
        "model.learn(total_timesteps = 100000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "LNE6YRKhi730"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparaci칩n de resultados:\n",
            "Pol칤tica Aleatoria \n",
            "Promedio: -208.50902921506776, Desviaci칩n: 143.06927422727438\n",
            "Pol칤tica PPO \n",
            "Promedio: -131.04745755518053, Desviaci칩n: 82.37868052944239\n",
            "Pol칤tica PPO Optimizado \n",
            "Promedio: 166.15260113088635, Desviaci칩n: 83.67149751757806\n"
          ]
        }
      ],
      "source": [
        "# Evaluar la pol칤tica aprendida por PPO\n",
        "ppo_opt_rewards = []\n",
        "\n",
        "for _ in range(n):\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    total = 0\n",
        "\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs, deterministic=True)  # Pol칤tica PPO\n",
        "        obs, reward, done, _, _ = env.step(action)\n",
        "        total += reward\n",
        "\n",
        "    ppo_opt_rewards.append(total)\n",
        "\n",
        "# Calcular promedio y desviaci칩n est치ndar de la pol칤tica PPO\n",
        "average_ppo_opt_reward = np.mean(ppo_opt_rewards)\n",
        "std_ppo_opt_deviation = np.std(ppo_opt_rewards)\n",
        "\n",
        "# Comparar resultados\n",
        "print(\"Comparaci칩n de resultados:\")\n",
        "print(f\"Pol칤tica Aleatoria \\nPromedio: {average_reward}, Desviaci칩n: {std_deviation}\")\n",
        "print(f\"Pol칤tica PPO \\nPromedio: {average_ppo_reward}, Desviaci칩n: {std_ppo_deviation}\")\n",
        "print(f\"Pol칤tica PPO Optimizado \\nPromedio: {average_ppo_opt_reward}, Desviaci칩n: {std_ppo_opt_deviation}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finalmente, a침adiendo m치s timesteps al entrenamiento, el modelo optimizado toma un promedio positivo, lo cual es un resultado bastante bueno en base a nuestro sistema de recompensas. Notamos que la desviaci칩n se mantiene similar al modelo sin optimizar, por lo que tiene una robustez similar en sus resultados (es una buena noticia tambi칠n, ya que no perdemos consistencia a cambio de mejores resultados promedio)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "cUy0eUMrjXZZ"
      },
      "outputs": [],
      "source": [
        "export_gif(model, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "Image(open('agent_performance.gif','rb').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Agent Performance](agent_performance.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPUY-Ktgf2BO"
      },
      "source": [
        "## **2. Large Language Models (4.0 puntos)**\n",
        "\n",
        "En esta secci칩n se enfocar치n en habilitar un Chatbot que nos permita responder preguntas 칰tiles a trav칠s de LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ4fPRRihGLe"
      },
      "source": [
        "### **2.0 Configuraci칩n Inicial**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media1.tenor.com/m/uqAs9atZH58AAAAd/config-config-issue.gif\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "Como siempre, cargamos todas nuestras API KEY al entorno:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Ud2Xm_k-hFJn"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
        "\n",
        "if \"TAVILY_API_KEY\" not in os.environ:\n",
        "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj9JvQUsgZZJ"
      },
      "source": [
        "### **2.1 Retrieval Augmented Generation (1.5 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://y.yarn.co/218aaa02-c47e-4ec9-b1c9-07792a06a88f_text.gif\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "El objetivo de esta subsecci칩n es que habiliten un chatbot que pueda responder preguntas usando informaci칩n contenida en documentos PDF a trav칠s de **Retrieval Augmented Generation.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrxOQroVnaZ5"
      },
      "source": [
        "#### **2.1.1 Reunir Documentos (0 puntos)**\n",
        "\n",
        "Reuna documentos PDF sobre los que hacer preguntas siguiendo las siguientes instrucciones:\n",
        "  - 2 documentos .pdf como m칤nimo.\n",
        "  - 50 p치ginas de contenido como m칤nimo entre todos los documentos.\n",
        "  - Ideas para documentos: Documentos relacionados a temas acad칠micos, laborales o de ocio. Aprovechen este ejercicio para construir algo 칰til y/o relevante para ustedes!\n",
        "  - Deben ocupar documentos reales, no pueden utilizar los mismos de la clase.\n",
        "  - Deben registrar sus documentos en la siguiente [planilla](https://docs.google.com/spreadsheets/d/1Hy1w_dOiG2UCHJ8muyxhdKPZEPrrL7BNHm6E90imIIM/edit?usp=sharing). **NO PUEDEN USAR LOS MISMOS DOCUMENTOS QUE OTRO GRUPO**\n",
        "  - **Recuerden adjuntar los documentos en su entrega**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5D1tIRCi4oJJ"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzq2TjWCnu15"
      },
      "outputs": [],
      "source": [
        "import PyPDF2\n",
        "\n",
        "doc_paths = [] # rellenar con los path a sus documentos\n",
        "\n",
        "assert len(doc_paths) >= 2, \"Deben adjuntar un m칤nimo de 2 documentos\"\n",
        "\n",
        "total_paginas = sum(len(PyPDF2.PdfReader(open(doc, \"rb\")).pages) for doc in doc_paths)\n",
        "assert total_paginas >= 50, f\"P치ginas insuficientes: {total_paginas}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r811-P71nizA"
      },
      "source": [
        "#### **2.1.2 Vectorizar Documentos (0.2 puntos)**\n",
        "\n",
        "Vectorice los documentos y almacene sus representaciones de manera acorde."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-yXAdCSn4JM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAUkP5zrnyBK"
      },
      "source": [
        "#### **2.1.3 Habilitar RAG (0.3 puntos)**\n",
        "\n",
        "Habilite la soluci칩n RAG a trav칠s de una *chain* y gu치rdela en una variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPIySdDFn99l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycg5S5i_n-kL"
      },
      "source": [
        "#### **2.1.4 Verificaci칩n de respuestas (0.5 puntos)**\n",
        "\n",
        "Genere un listado de 3 tuplas (\"pregunta\", \"respuesta correcta\") y analice la respuesta de su soluci칩n para cada una. 쯉u soluci칩n RAG entrega las respuestas que esperaba?\n",
        "\n",
        "Ejemplo de tupla:\n",
        "- Pregunta: 쯈ui칠n es el presidente de Chile?\n",
        "- Respuesta correcta: El presidente de Chile es Gabriel Boric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_UiEn1hoZYR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8d5zTMHoUgF"
      },
      "source": [
        "#### **2.1.5 Sensibilidad de Hiperpar치metros (0.5 puntos)**\n",
        "\n",
        "Extienda el an치lisis del punto 2.1.4 analizando c칩mo cambian las respuestas entregadas cambiando los siguientes hiperpar치metros:\n",
        "- `Tama침o del chunk`. (*쮺칩mo repercute que los chunks sean mas grandes o chicos?*)\n",
        "- `La cantidad de chunks recuperados`. (*쯈u칠 pasa si se devuelven muchos/pocos chunks?*)\n",
        "- `El tipo de b칰squeda`. (*쮺칩mo afecta el tipo de b칰squeda a las respuestas de mi RAG?*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDh_QgeXLGHc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENJiPPM0giX8"
      },
      "source": [
        "### **2.2 Agentes (1.0 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media1.tenor.com/m/rcqnN2aJCSEAAAAd/secret-agent-man.gif\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "Similar a la secci칩n anterior, en esta secci칩n se busca habilitar **Agentes** para obtener informaci칩n a trav칠s de tools y as칤 responder la pregunta del usuario."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V47l7Mjfrk0N"
      },
      "source": [
        "#### **2.2.1 Tool de Tavily (0.2 puntos)**\n",
        "\n",
        "Generar una *tool* que pueda hacer consultas al motor de b칰squeda **Tavily**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6SLKwcWr0AG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SonB1A-9rtRq"
      },
      "source": [
        "#### **2.2.2 Tool de Wikipedia (0.2 puntos)**\n",
        "\n",
        "Generar una *tool* que pueda hacer consultas a **Wikipedia**.\n",
        "\n",
        "*Hint: Le puede ser de ayuda el siguiente [link](https://python.langchain.com/v0.1/docs/modules/tools/).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehJJpoqsr26-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvUIMdX6r0ne"
      },
      "source": [
        "#### **2.2.3 Crear Agente (0.3 puntos)**\n",
        "\n",
        "Crear un agente que pueda responder preguntas preguntas usando las *tools* antes generadas. Aseg칰rese que su agente responda en espa침ol. Por 칰ltimo, guarde el agente en una variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pD1_n0wrsDI5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKV0JxK3r-XG"
      },
      "source": [
        "#### **2.2.4 Verificaci칩n de respuestas (0.3 puntos)**\n",
        "\n",
        "Pruebe el funcionamiento de su agente y aseg칰rese que el agente est칠 ocupando correctamente las tools disponibles. 쮼n qu칠 casos el agente deber칤a ocupar la tool de Tavily? 쮼n qu칠 casos deber칤a ocupar la tool de Wikipedia?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pqo2dsxvywW_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZbDTYiogquv"
      },
      "source": [
        "### **2.3 Multi Agente (1.5 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media1.tenor.com/m/r7QMJLxU4BoAAAAd/this-is-getting-out-of-hand-star-wars.gif\"\n",
        "\" width=\"450\">\n",
        "</p>\n",
        "\n",
        "El objetivo de esta subsecci칩n es encapsular las funcionalidades creadas en una soluci칩n multiagente con un **supervisor**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-iUfH0WvI6m"
      },
      "source": [
        "#### **2.3.1 Generando Tools (0.5 puntos)**\n",
        "\n",
        "Transforme la soluci칩n RAG de la secci칩n 2.1 y el agente de la secci칩n 2.2 a *tools* (una tool por cada uno)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw1cfTtvv1AZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQYNjT_0vPCg"
      },
      "source": [
        "#### **2.3.2 Agente Supervisor (0.5 puntos)**\n",
        "\n",
        "Habilite un agente que tenga acceso a las tools del punto anterior y pueda responder preguntas relacionadas. Almacene este agente en una variable llamada supervisor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv2ZY0BAv1RD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea3zWlvyvY7K"
      },
      "source": [
        "#### **2.3.3 Verificaci칩n de respuestas (0.25 puntos)**\n",
        "\n",
        "Pruebe el funcionamiento de su agente repitiendo las preguntas realizadas en las secciones 2.1.4 y 2.2.4 y comente sus resultados. 쮺칩mo var칤an las respuestas bajo este enfoque?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_1t0zkgv1qW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb8bdAmYvgwn"
      },
      "source": [
        "#### **2.3.4 An치lisis (0.25 puntos)**\n",
        "\n",
        "쯈u칠 diferencias tiene este enfoque con la soluci칩n *Router* vista en clases? Nombre al menos una ventaja y desventaja."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAUlJxqoLK5r"
      },
      "source": [
        "`escriba su respuesta ac치`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JWVSuWiZ8Mj"
      },
      "source": [
        "### **2.4 Memoria (Bonus +0.5 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media1.tenor.com/m/Gs95aiElrscAAAAd/memory-unlocked-ratatouille-critic.gif\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "Una de las principales falencias de las soluciones que hemos visto hasta ahora es que nuestro chat no responde las interacciones anteriores, por ejemplo:\n",
        "\n",
        "- Pregunta 1: \"Hola! mi nombre es Sebasti치n\"\n",
        "  - Respuesta esperada: \"Hola Sebasti치n! ...\"\n",
        "- Pregunta 2: \"Cual es mi nombre?\"\n",
        "  - Respuesta actual: \"Lo siento pero no conozco tu nombre :(\"\n",
        "  - **Respuesta esperada: \"Tu nombre es Sebasti치n\"**\n",
        "\n",
        "Para solucionar esto, se les solicita agregar un componente de **memoria** a la soluci칩n entregada en el punto 2.3.\n",
        "\n",
        "**Nota: El Bonus es v치lido <u>s칩lo para la secci칩n 2 de Large Language Models.</u>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6Y7tIPJLPfB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFc3jBT5g0kT"
      },
      "source": [
        "### **2.5 Despliegue (0 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media1.tenor.com/m/IytHqOp52EsAAAAd/you-get-a-deploy-deploy.gif\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "Una vez tengan los puntos anteriores finalizados, toca la etapa de dar a conocer lo que hicimos! Para eso, vamos a desplegar nuestro modelo a trav칠s de `gradio`, una librer칤a especializada en el levantamiento r치pido de demos basadas en ML.\n",
        "\n",
        "Primero instalamos la librer칤a:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8TsvnCPbkIA"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJBztEUovKsF"
      },
      "source": [
        "Luego s칩lo deben ejecutar el siguiente c칩digo e interactuar con la interfaz a trav칠s del notebook o del link generado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3KedQSvg1-n"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import time\n",
        "\n",
        "def agent_response(message, history):\n",
        "  '''\n",
        "  Funci칩n para gradio, recibe mensaje e historial, devuelte la respuesta del chatbot.\n",
        "  '''\n",
        "  # get chatbot response\n",
        "  response = ... # rellenar con la respuesta de su chat\n",
        "\n",
        "  # assert\n",
        "  assert type(response) == str, \"output de route_question debe ser string\"\n",
        "\n",
        "  # \"streaming\" response\n",
        "  for i in range(len(response)):\n",
        "    time.sleep(0.015)\n",
        "    yield response[: i+1]\n",
        "\n",
        "gr.ChatInterface(\n",
        "    agent_response,\n",
        "    type=\"messages\",\n",
        "    title=\"Chatbot MDS7202\", # Pueden cambiar esto si lo desean\n",
        "    description=\"Hola! Soy un chatbot muy 칰til :)\", # tambi칠n la descripci칩n\n",
        "    theme=\"soft\",\n",
        "    ).launch(\n",
        "        share=True, # pueden compartir el link a sus amig@s para que interactuen con su chat!\n",
        "        debug = False,\n",
        "        )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
