{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyPTffTLug7i"
      },
      "source": [
        "# **Laboratorio 11: LLM y Agentes Autónomos 🤖**\n",
        "\n",
        "MDS7202: Laboratorio de Programación Científica para Ciencia de Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pbWVyntzbvL"
      },
      "source": [
        "### **Cuerpo Docente:**\n",
        "\n",
        "- Profesores: Ignacio Meza, Sebastián Tinoco\n",
        "- Auxiliar: Eduardo Moya\n",
        "- Ayudantes: Nicolás Ojeda, Melanie Peña, Valentina Rojas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dy6ikgVYzghB"
      },
      "source": [
        "### Equipo: SUPER IMPORTANTE - notebooks sin nombre no serán revisados\n",
        "\n",
        "- Nombre de alumno 1: Martín Torrico\n",
        "- Nombre de alumno 2: Alejandra Toro\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMJ-owchzjFf"
      },
      "source": [
        "### **Link de repositorio de GitHub:** [Insertar Repositorio](https://github.com/MartinTorricoP/Laboratorios_MDS7202)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUuwsXrKzmkK"
      },
      "source": [
        "## **Temas a tratar**\n",
        "\n",
        "- Reinforcement Learning\n",
        "- Large Language Models\n",
        "\n",
        "## **Reglas:**\n",
        "\n",
        "- **Grupos de 2 personas**\n",
        "- Cualquier duda fuera del horario de clases al foro. Mensajes al equipo docente serán respondidos por este medio.\n",
        "- Prohibidas las copias.\n",
        "- Pueden usar cualquer matrial del curso que estimen conveniente.\n",
        "\n",
        "### **Objetivos principales del laboratorio**\n",
        "\n",
        "- Resolución de problemas secuenciales usando Reinforcement Learning\n",
        "- Habilitar un Chatbot para entregar respuestas útiles usando Large Language Models.\n",
        "\n",
        "El laboratorio deberá ser desarrollado sin el uso indiscriminado de iteradores nativos de python (aka \"for\", \"while\"). La idea es que aprendan a exprimir al máximo las funciones optimizadas que nos entrega `pandas`, las cuales vale mencionar, son bastante más eficientes que los iteradores nativos sobre DataFrames."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hmHHQ9BuyAG"
      },
      "source": [
        "## **1. Reinforcement Learning (2.0 puntos)**\n",
        "\n",
        "En esta sección van a usar métodos de RL para resolver dos problemas interesantes: `Blackjack` y `LunarLander`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gOcejYb6uzOO"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (box2d-py)\n"
          ]
        }
      ],
      "source": [
        "!pip install -qqq gymnasium stable_baselines3\n",
        "!pip install -qqq swig\n",
        "!pip install -qqq gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBPet_Mq8dX9"
      },
      "source": [
        "### **1.1 Blackjack (1.0 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://www.recreoviral.com/wp-content/uploads/2016/08/s3.amazonaws.com-Math.gif\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "La idea de esta subsección es que puedan implementar métodos de RL y así generar una estrategia para jugar el clásico juego Blackjack y de paso puedan ~~hacerse millonarios~~ aprender a resolver problemas mediante RL.\n",
        "\n",
        "Comencemos primero preparando el ambiente. El siguiente bloque de código transforma las observaciones del ambiente a `np.array`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LpZ8bBKk9ZlU"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium.spaces import MultiDiscrete\n",
        "import numpy as np\n",
        "\n",
        "class FlattenObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(FlattenObservation, self).__init__(env)\n",
        "        self.observation_space = MultiDiscrete(np.array([32, 11, 2]))\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.array(observation).flatten()\n",
        "\n",
        "# Create and wrap the environment\n",
        "env = gym.make(\"Blackjack-v1\")\n",
        "env = FlattenObservation(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ6J1_-Y9nHO"
      },
      "source": [
        "#### **1.1.1 Descripción de MDP (0.2 puntos)**\n",
        "\n",
        "Entregue una breve descripción sobre el ambiente [Blackjack](https://gymnasium.farama.org/environments/toy_text/blackjack/) y su formulación en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5i1Wt1p770x"
      },
      "source": [
        "\n",
        "El ambiente de Blackjack adjunto consiste en una partida de Blackjack, juego donde el objetivo es que la suma de tus cartas se mayor a la del dealer sin excederse de los 21 puntos. La formulación en MDP corresponde a :\n",
        "\n",
        "* Estados:\n",
        "  - Suma actual de las cartas del jugador: Valores entre el 4 al 21.\n",
        "  - Carta visible del dealer: Valores del 1 al 10 (donde 1 es un As).\n",
        "  - As utilizable: Valor 0 o 1 (siendo 1 cuando el jugador puede utilizar un As).\n",
        "\n",
        "* Acciones:\n",
        " - Quedarse con la suma actual: 0 (stick).\n",
        " - Pedir otra carta: 1 (hit).\n",
        "\n",
        "* Recompensas:\n",
        " - Ganar: +1.\n",
        " - Perder: -1.\n",
        " - Empatar: 0.\n",
        " - Ganar con blackjack natural: +1,5 (solo si natural=True).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmcX6bRC9agQ"
      },
      "source": [
        "#### **1.1.2 Generando un Baseline (0.2 puntos)**\n",
        "\n",
        "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulación 5000 veces y reporte el promedio y desviación de las recompensas. ¿Cómo calificaría el performance de esta política? ¿Cómo podría interpretar las recompensas obtenidas?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9p2PrLLR9yju",
        "outputId": "a81d36e8-3614-40df-ef05-1cd41829f012"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Promedio de las recompensas: -0.407\n",
            "Desviación estándar de las recompensas: 0.8922729403047029\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "# Variables para almacenar resultados\n",
        "rewards = []\n",
        "n = 5000 #numero de repeticiones\n",
        "\n",
        "# Simulación de 5000 episodios con acciones aleatorias\n",
        "for _ in range(n):\n",
        "    obs = env.reset()  # Reiniciar el ambiente\n",
        "    done = False\n",
        "    total = 0\n",
        "\n",
        "    while not done:\n",
        "        action = env.action_space.sample() # Seleccionar una acción aleatoria (0 o 1)\n",
        "        obs, reward, done, _, _ = env.step(action) #Ejecutar la acción\n",
        "        total += reward\n",
        "\n",
        "    rewards.append(total)\n",
        "\n",
        "# Calcular promedio y desviación estándar de las recompensas\n",
        "average_reward = np.mean(rewards)\n",
        "std_deviation = np.std(rewards)\n",
        "\n",
        "print(f\"Promedio de las recompensas: {average_reward}\")\n",
        "print(f\"Desviación estándar de las recompensas: {std_deviation}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq-VAb7ZBksa"
      },
      "source": [
        "El desempeño de la política aleatoria es subóptimo.\n",
        "\n",
        "Esto es, ya que contamos con un promedio de recompensas negativos (-0.407), indicando más pérdidas que ganancias, sumado a una alta variabilidad con una desviación estándar de 0.8922, debido a la naturaleza estocástica del juego.\n",
        "\n",
        "Con esta política, podemos tener un resultado base el cual podemos mejorar, mostrandonos que no es un juego donde una buena estrategia es similar a lanzar una moneda, sino que la estrategia de cuándo pedir o cuándo quedarse es importante para tener mejores resultados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEO_dY4x_SJu"
      },
      "source": [
        "#### **1.1.3 Entrenamiento de modelo (0.2 puntos)**\n",
        "\n",
        "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `Blackjack`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9JsFA1wGmnH",
        "outputId": "73d8c12a-4dfc-4022-e7df-f212f60f5f88"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import DQN\n",
        "\n",
        "# Crear el modelo DQN\n",
        "model = DQN(\"MlpPolicy\", env, verbose=1)\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.learn(total_timesteps=5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-bpdb8wZID1"
      },
      "source": [
        "#### **1.1.4 Evaluación de modelo (0.2 puntos)**\n",
        "\n",
        "Repita el ejercicio 1.1.2 pero utilizando el modelo entrenado. ¿Cómo es el performance de su agente? ¿Es mejor o peor que el escenario baseline?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-d7d8GFf7F6",
        "outputId": "ed9c4288-5793-4899-efc5-faa497ba276e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparación de resultados:\n",
            "Política Aleatoria \n",
            "Promedio: -0.407, Desviación: 0.8922729403047029\n",
            "Política DQN \n",
            "Promedio: -0.1144, Desviación: 0.9521095735260726\n"
          ]
        }
      ],
      "source": [
        "# Evaluar la política aprendida por DQN\n",
        "dqn_rewards = []\n",
        "\n",
        "for _ in range(n):\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    total = 0\n",
        "\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs, deterministic=True)  # Política DQN\n",
        "        obs, reward, done, _, _ = env.step(action)\n",
        "        total += reward\n",
        "\n",
        "    dqn_rewards.append(total)\n",
        "\n",
        "# Calcular promedio y desviación estándar de la política DQN\n",
        "average_dqn_reward = np.mean(dqn_rewards)\n",
        "std_dqn_deviation = np.std(dqn_rewards)\n",
        "\n",
        "# Comparar resultados\n",
        "print(\"Comparación de resultados:\")\n",
        "print(f\"Política Aleatoria \\nPromedio: {average_reward}, Desviación: {std_deviation}\")\n",
        "print(f\"Política DQN \\nPromedio: {average_dqn_reward}, Desviación: {std_dqn_deviation}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLVoKZq_Hz_e"
      },
      "source": [
        "Podemos ver que el desempeño del agente mejora considerablemente en cuanto el promedio de sus recompensas, subiendo su promedio a -0.1144, tomando deciciones estratégicas e inteligentes y con ello ganando más veces que tomando deciciones al azar.\n",
        "\n",
        "Sin embargo, podemos ver también que el desempeño del agente tiene más variabilidad en sus resultados con su política, lo cual nos indica que puede ser refinado para mejorar su toma de decisiones.\n",
        "\n",
        "Con lo anterior, este escenario es mejor al aleatorio, pero sería recomendable seguir mejorando su consistencia en el desempeño.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO-EsAaPAYEm"
      },
      "source": [
        "#### **1.1.5 Estudio de acciones (0.2 puntos)**\n",
        "\n",
        "Genere una función que reciba un estado y retorne la accion del agente. Luego, use esta función para entregar la acción escogida frente a los siguientes escenarios:\n",
        "\n",
        "- Suma de cartas del agente es 6, dealer muestra un 7, agente no tiene tiene un as\n",
        "- Suma de cartas del agente es 19, dealer muestra un 3, agente tiene tiene un as\n",
        "\n",
        "¿Son coherentes sus acciones con las reglas del juego?\n",
        "\n",
        "Hint: ¿A que clase de python pertenecen los estados? Pruebe a usar el método `.reset` para saberlo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Fh8XlGyzwtRp"
      },
      "outputs": [],
      "source": [
        "def elegir_accion(estado, model):\n",
        "    action, _ = model.predict(estado, deterministic=True)\n",
        "    return action\n",
        "\n",
        "estado1 = [6, 7, False]\n",
        "accion1 = elegir_accion(estado1, model)\n",
        "\n",
        "estado2 = [19, 3, True]\n",
        "accion2 = elegir_accion(estado2, model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOkRbjLULS03",
        "outputId": "b9286ba9-0525-46fa-d4f9-4184dd304bf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acción para el estado 1 (6, 7, sin as): Quedarse\n"
          ]
        }
      ],
      "source": [
        "estado_1 = [6, 7, False]\n",
        "accion_1 = elegir_accion(estado_1, model)\n",
        "\n",
        "print(f\"Acción para el estado 1 (6, 7, sin as): {'Pedir carta' if accion_1 == 1 else 'Quedarse'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BeKEBC9LwiE",
        "outputId": "10525308-26c0-47c7-c5e8-6e646c2495d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acción para el estado 2 (19, 3, con as): Quedarse\n"
          ]
        }
      ],
      "source": [
        "estado_2 = [19, 3, True]\n",
        "accion_2 = elegir_accion(estado_2, model)\n",
        "\n",
        "print(f\"Acción para el estado 2 (19, 3, con as): {'Pedir carta' if accion_2 == 1 else 'Quedarse'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3P9QvGS0SIDw"
      },
      "source": [
        "En este caso, solo el estado 2 tiene sentido, mientras que el estado 1 no tiene sentido, por lo que sería interesante poder optimizar el modelo.\n",
        "\n",
        "Esto es porque la suma en el primer caso del agente es muy baja y sería normal pedir una sigiente carta (si no pedimos, prácticamente perdemos directamente), mientras que en el estado 2 la suma del jugador es muy alta y seguramente sobrepase el 21 (y actualmente, la carta que tiene el dealer es muy baja).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEqCTqqroh03"
      },
      "source": [
        "### **1.2 LunarLander**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.redd.it/097t6tk29zf51.jpg\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "Similar a la sección 2.1, en esta sección usted se encargará de implementar una gente de RL que pueda resolver el ambiente `LunarLander`.\n",
        "\n",
        "Comencemos preparando el ambiente:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvQUyuZ_FtZ4",
        "outputId": "b0a8ac67-86f7-441d-acc8-b2146390b8ff"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "env = gym.make(\"LunarLander-v3\", render_mode = \"rgb_array\", continuous = True) # notar el parámetro continuous = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBU4lGX3wpN6"
      },
      "source": [
        "Noten que se especifica el parámetro `continuous = True`. ¿Que implicancias tiene esto sobre el ambiente?\n",
        "\n",
        "Además, se le facilita la función `export_gif` para el ejercicio 2.2.4:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRiWpSo9yfr9",
        "outputId": "38f87a3b-8a34-4c2a-b961-e54e0c71ac07"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "import numpy as np\n",
        "\n",
        "def export_gif(model, n = 5):\n",
        "  '''\n",
        "  función que exporta a gif el comportamiento del agente en n episodios\n",
        "  '''\n",
        "  images = []\n",
        "  for episode in range(n):\n",
        "    obs = model.env.reset()\n",
        "    img = model.env.render()\n",
        "    done = False\n",
        "    while not done:\n",
        "      images.append(img)\n",
        "      action, _ = model.predict(obs)\n",
        "      obs, reward, done, info = model.env.step(action)\n",
        "      img = model.env.render(mode=\"rgb_array\")\n",
        "\n",
        "  imageio.mimsave(\"agent_performance.gif\", [np.array(img) for i, img in enumerate(images) if i % 5 == 0], fps=15) # editado para mejor rendimiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El parámetro continuous=True transforma el problema en un desafío más complejo al permitir un control más detallado del lander, lo cual también requiere métodos de aprendizaje más sofisticados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk5VJVppXh3N"
      },
      "source": [
        "#### **1.2.1 Descripción de MDP (0.2 puntos)**\n",
        "\n",
        "Entregue una breve descripción sobre el ambiente [LunarLander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) y su formulación en MDP, distinguiendo de forma clara y concisa los estados, acciones y recompensas. ¿Como se distinguen las acciones de este ambiente en comparación a `Blackjack`?\n",
        "\n",
        "Nota: recuerde que se especificó el parámetro `continuous = True`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb-u9LUE8O9a"
      },
      "source": [
        "El ambiente de LunarLander consiste en un problema de control de trayectoria de un cohete, donde el objetivo es aterrizar en una plataforma sin salir del área designada ni estrellarse. La formulación en MDP corresponde a:\n",
        "\n",
        "* Estados:\n",
        "  - Coordenadas x e y del lander (posición relativa al área de aterrizaje).\n",
        "  - Velocidades lineales en x e y.\n",
        "  - Ángulo de orientación del lander.\n",
        "  - Velocidad angular.\n",
        "  - Contacto de las patas con el suelo (dos valores booleanos).\n",
        "\n",
        "* Acciones:\n",
        "  - 0: No realizar ninguna acción.\n",
        "  - 1: Activar el motor lateral izquierdo.\n",
        "  - 2: Activar el motor principal.\n",
        "  - 3: Activar el motor lateral derecho.\n",
        "\n",
        "* Recompensas:\n",
        "- Proximidad al área de aterrizaje: Incrementos/penalizaciones por acercarse/alejarse.\n",
        "- Velocidad y orientación:\n",
        "  - Penalización por velocidades altas y ángulos inadecuados.\n",
        "- Uso de motores:\n",
        "  - Penalización por activaciones de motores -0,03 para laterales y -0,3 para el principal.\n",
        "- Aterrizajes:\n",
        "  - Recompensa de +10 por cada pata en contacto con el suelo.\n",
        "  - Recompensa de +100 por un aterrizaje exitoso.\n",
        "  - Penalización de -100 por estrellarse.\n",
        "  - Un episodio se considera resuelto si el agente acumula al menos 200 puntos.\n",
        "\n",
        "**Diferencia de acciones con Blackjack**:\n",
        "\n",
        "A diferencia de Blackjack, donde las acciones son discretas y binarias (pedir o quedarse), en LunarLander las acciones pueden ser discretas (selección de motores) o **continuas** (intensidad de empuje), dependiendo de la configuración del ambiente.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YChodtNQwzG2"
      },
      "source": [
        "#### **1.2.2 Generando un Baseline (0.2 puntos)**\n",
        "\n",
        "Simule un escenario en donde se escojan acciones aleatorias. Repita esta simulación 10 veces y reporte el promedio y desviación de las recompensas. ¿Cómo calificaría el performance de esta política?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bwc3A0GX7a8",
        "outputId": "a1785b3b-9157-4d1f-8e6c-ba4f91f041e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Promedio de las recompensas: -208.50902921506776\n",
            "Desviación estándar de las recompensas: 143.06927422727438\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "# Variables para almacenar resultados\n",
        "rewards = []\n",
        "n = 10  # número de repeticiones\n",
        "\n",
        "# Simulación de 10 episodios con acciones aleatorias\n",
        "for _ in range(n):\n",
        "    obs = env.reset()  # Reiniciar el ambiente\n",
        "    done = False\n",
        "    total = 0\n",
        "\n",
        "    while not done:\n",
        "        action = env.action_space.sample()  # Seleccionar una acción aleatoria\n",
        "        obs, reward, done, _, _ = env.step(action)  # Ejecutar la acción\n",
        "        total += reward\n",
        "\n",
        "    rewards.append(total)\n",
        "\n",
        "# Calcular promedio y desviación estándar de las recompensas\n",
        "average_reward = np.mean(rewards)\n",
        "std_deviation = np.std(rewards)\n",
        "\n",
        "print(f\"Promedio de las recompensas: {average_reward}\")\n",
        "print(f\"Desviación estándar de las recompensas: {std_deviation}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf5HBRLRZq3h"
      },
      "source": [
        "El rendimiento de esta política aleatoria nuevamente es subóptima como es esperado.\n",
        "\n",
        "Podemos ver que en promedio se estrella el lander 2 veces por aterrizaje, lo cual es un muy mal desempeño para lo que queremos y que tiene una desviación estándar muy alta de sus recompenzas, con un valor de 143.\n",
        "\n",
        "Claramente hay espacio de mejora y lo desarrollaremos a continuación:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQrZVQflX_5f"
      },
      "source": [
        "#### **1.2.3 Entrenamiento de modelo (0.2 puntos)**\n",
        "\n",
        "A partir del siguiente [enlace](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html), escoja un modelo de `stable_baselines3` y entrenelo para resolver el ambiente `LunarLander` **usando 10000 timesteps de entrenamiento**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_6Ia9uoF7Hs",
        "outputId": "e28152bc-ef69-4c37-d18b-8ed14cd9f564"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO\n",
        "\n",
        "# Crear el modelo PPO\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.learn(total_timesteps=10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z-oIUSrlAsY"
      },
      "source": [
        "#### **1.2.4 Evaluación de modelo (0.2 puntos)**\n",
        "\n",
        "Repita el ejercicio 1.2.2 pero utilizando el modelo entrenado. ¿Cómo es el performance de su agente? ¿Es mejor o peor que el escenario baseline?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ophyU3KrWrwl",
        "outputId": "323f76c6-d17e-4d5a-acda-e207173eadf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparación de resultados:\n",
            "Política Aleatoria \n",
            "Promedio: -208.50902921506776, Desviación: 143.06927422727438\n",
            "Política PPO \n",
            "Promedio: -131.04745755518053, Desviación: 82.37868052944239\n"
          ]
        }
      ],
      "source": [
        "# Evaluar la política aprendida por PPO\n",
        "ppo_rewards = []\n",
        "\n",
        "for _ in range(n):\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    total = 0\n",
        "\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs, deterministic=True)  # Política PPO\n",
        "        obs, reward, done, _, _ = env.step(action)\n",
        "        total += reward\n",
        "\n",
        "    ppo_rewards.append(total)\n",
        "\n",
        "# Calcular promedio y desviación estándar de la política PPO\n",
        "average_ppo_reward = np.mean(ppo_rewards)\n",
        "std_ppo_deviation = np.std(ppo_rewards)\n",
        "\n",
        "# Comparar resultados\n",
        "print(\"Comparación de resultados:\")\n",
        "print(f\"Política Aleatoria \\nPromedio: {average_reward}, Desviación: {std_deviation}\")\n",
        "print(f\"Política PPO \\nPromedio: {average_ppo_reward}, Desviación: {std_ppo_deviation}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qf8TctocMme"
      },
      "source": [
        "El performance de la gente nuevamente mejora, pero no de manera tan sustancial como en el caso del blackjack. En este caso, podemos ver que en promedio se estrella 1 vez el lander en vez de 2, lo cual es bueno, pero no óptimo. Además, viendo que su desviación tiene un valor menor, sabemos que este resultado es más consistente en el tiempo. Con esto, tenemos un mejor modelo que el baseline, pero con espacio a mejora."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6Xw4YHT3P5d"
      },
      "source": [
        "#### **1.2.5 Optimización de modelo (0.2 puntos)**\n",
        "\n",
        "Repita los ejercicios 1.2.3 y 1.2.4 hasta obtener un nivel de recompensas promedio mayor a 50. Para esto, puede cambiar manualmente parámetros como:\n",
        "- `total_timesteps`\n",
        "- `learning_rate`\n",
        "- `batch_size`\n",
        "\n",
        "Una vez optimizado el modelo, use la función `export_gif` para estudiar el comportamiento de su agente en la resolución del ambiente y comente sobre sus resultados.\n",
        "\n",
        "Adjunte el gif generado en su entrega (mejor aún si además adjuntan el gif en el markdown)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aItYF6sr6F_6"
      },
      "outputs": [],
      "source": [
        "model = PPO(\"MlpPolicy\", env, verbose=1, seed = 123)\n",
        "model.learn(total_timesteps = 100000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "LNE6YRKhi730"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparación de resultados:\n",
            "Política Aleatoria \n",
            "Promedio: -208.50902921506776, Desviación: 143.06927422727438\n",
            "Política PPO \n",
            "Promedio: -131.04745755518053, Desviación: 82.37868052944239\n",
            "Política PPO Optimizado \n",
            "Promedio: 166.15260113088635, Desviación: 83.67149751757806\n"
          ]
        }
      ],
      "source": [
        "# Evaluar la política aprendida por PPO\n",
        "ppo_opt_rewards = []\n",
        "\n",
        "for _ in range(n):\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    total = 0\n",
        "\n",
        "    while not done:\n",
        "        action, _ = model.predict(obs, deterministic=True)  # Política PPO\n",
        "        obs, reward, done, _, _ = env.step(action)\n",
        "        total += reward\n",
        "\n",
        "    ppo_opt_rewards.append(total)\n",
        "\n",
        "# Calcular promedio y desviación estándar de la política PPO\n",
        "average_ppo_opt_reward = np.mean(ppo_opt_rewards)\n",
        "std_ppo_opt_deviation = np.std(ppo_opt_rewards)\n",
        "\n",
        "# Comparar resultados\n",
        "print(\"Comparación de resultados:\")\n",
        "print(f\"Política Aleatoria \\nPromedio: {average_reward}, Desviación: {std_deviation}\")\n",
        "print(f\"Política PPO \\nPromedio: {average_ppo_reward}, Desviación: {std_ppo_deviation}\")\n",
        "print(f\"Política PPO Optimizado \\nPromedio: {average_ppo_opt_reward}, Desviación: {std_ppo_opt_deviation}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finalmente, añadiendo más timesteps al entrenamiento, el modelo optimizado toma un promedio positivo, lo cual es un resultado bastante bueno en base a nuestro sistema de recompensas. Notamos que la desviación se mantiene similar al modelo sin optimizar, por lo que tiene una robustez similar en sus resultados (es una buena noticia también, ya que no perdemos consistencia a cambio de mejores resultados promedio)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "cUy0eUMrjXZZ"
      },
      "outputs": [],
      "source": [
        "export_gif(model, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "Image(open('agent_performance.gif','rb').read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Agent Performance](agent_performance.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPUY-Ktgf2BO"
      },
      "source": [
        "## **2. Large Language Models (4.0 puntos)**\n",
        "\n",
        "En esta sección se enfocarán en habilitar un Chatbot que nos permita responder preguntas útiles a través de LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQ4fPRRihGLe"
      },
      "source": [
        "### **2.0 Configuración Inicial**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media1.tenor.com/m/uqAs9atZH58AAAAd/config-config-issue.gif\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "Como siempre, cargamos todas nuestras API KEY al entorno:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Ud2Xm_k-hFJn"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")\n",
        "\n",
        "if \"TAVILY_API_KEY\" not in os.environ:\n",
        "    os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj9JvQUsgZZJ"
      },
      "source": [
        "### **2.1 Retrieval Augmented Generation (1.5 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://y.yarn.co/218aaa02-c47e-4ec9-b1c9-07792a06a88f_text.gif\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "El objetivo de esta subsección es que habiliten un chatbot que pueda responder preguntas usando información contenida en documentos PDF a través de **Retrieval Augmented Generation.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrxOQroVnaZ5"
      },
      "source": [
        "#### **2.1.1 Reunir Documentos (0 puntos)**\n",
        "\n",
        "Reuna documentos PDF sobre los que hacer preguntas siguiendo las siguientes instrucciones:\n",
        "  - 2 documentos .pdf como mínimo.\n",
        "  - 50 páginas de contenido como mínimo entre todos los documentos.\n",
        "  - Ideas para documentos: Documentos relacionados a temas académicos, laborales o de ocio. Aprovechen este ejercicio para construir algo útil y/o relevante para ustedes!\n",
        "  - Deben ocupar documentos reales, no pueden utilizar los mismos de la clase.\n",
        "  - Deben registrar sus documentos en la siguiente [planilla](https://docs.google.com/spreadsheets/d/1Hy1w_dOiG2UCHJ8muyxhdKPZEPrrL7BNHm6E90imIIM/edit?usp=sharing). **NO PUEDEN USAR LOS MISMOS DOCUMENTOS QUE OTRO GRUPO**\n",
        "  - **Recuerden adjuntar los documentos en su entrega**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5D1tIRCi4oJJ"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzq2TjWCnu15"
      },
      "outputs": [],
      "source": [
        "import PyPDF2\n",
        "\n",
        "doc_paths = [] # rellenar con los path a sus documentos\n",
        "\n",
        "assert len(doc_paths) >= 2, \"Deben adjuntar un mínimo de 2 documentos\"\n",
        "\n",
        "total_paginas = sum(len(PyPDF2.PdfReader(open(doc, \"rb\")).pages) for doc in doc_paths)\n",
        "assert total_paginas >= 50, f\"Páginas insuficientes: {total_paginas}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r811-P71nizA"
      },
      "source": [
        "#### **2.1.2 Vectorizar Documentos (0.2 puntos)**\n",
        "\n",
        "Vectorice los documentos y almacene sus representaciones de manera acorde."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-yXAdCSn4JM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAUkP5zrnyBK"
      },
      "source": [
        "#### **2.1.3 Habilitar RAG (0.3 puntos)**\n",
        "\n",
        "Habilite la solución RAG a través de una *chain* y guárdela en una variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPIySdDFn99l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycg5S5i_n-kL"
      },
      "source": [
        "#### **2.1.4 Verificación de respuestas (0.5 puntos)**\n",
        "\n",
        "Genere un listado de 3 tuplas (\"pregunta\", \"respuesta correcta\") y analice la respuesta de su solución para cada una. ¿Su solución RAG entrega las respuestas que esperaba?\n",
        "\n",
        "Ejemplo de tupla:\n",
        "- Pregunta: ¿Quién es el presidente de Chile?\n",
        "- Respuesta correcta: El presidente de Chile es Gabriel Boric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_UiEn1hoZYR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8d5zTMHoUgF"
      },
      "source": [
        "#### **2.1.5 Sensibilidad de Hiperparámetros (0.5 puntos)**\n",
        "\n",
        "Extienda el análisis del punto 2.1.4 analizando cómo cambian las respuestas entregadas cambiando los siguientes hiperparámetros:\n",
        "- `Tamaño del chunk`. (*¿Cómo repercute que los chunks sean mas grandes o chicos?*)\n",
        "- `La cantidad de chunks recuperados`. (*¿Qué pasa si se devuelven muchos/pocos chunks?*)\n",
        "- `El tipo de búsqueda`. (*¿Cómo afecta el tipo de búsqueda a las respuestas de mi RAG?*)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDh_QgeXLGHc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENJiPPM0giX8"
      },
      "source": [
        "### **2.2 Agentes (1.0 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media1.tenor.com/m/rcqnN2aJCSEAAAAd/secret-agent-man.gif\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "Similar a la sección anterior, en esta sección se busca habilitar **Agentes** para obtener información a través de tools y así responder la pregunta del usuario."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V47l7Mjfrk0N"
      },
      "source": [
        "#### **2.2.1 Tool de Tavily (0.2 puntos)**\n",
        "\n",
        "Generar una *tool* que pueda hacer consultas al motor de búsqueda **Tavily**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6SLKwcWr0AG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SonB1A-9rtRq"
      },
      "source": [
        "#### **2.2.2 Tool de Wikipedia (0.2 puntos)**\n",
        "\n",
        "Generar una *tool* que pueda hacer consultas a **Wikipedia**.\n",
        "\n",
        "*Hint: Le puede ser de ayuda el siguiente [link](https://python.langchain.com/v0.1/docs/modules/tools/).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehJJpoqsr26-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvUIMdX6r0ne"
      },
      "source": [
        "#### **2.2.3 Crear Agente (0.3 puntos)**\n",
        "\n",
        "Crear un agente que pueda responder preguntas preguntas usando las *tools* antes generadas. Asegúrese que su agente responda en español. Por último, guarde el agente en una variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pD1_n0wrsDI5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKV0JxK3r-XG"
      },
      "source": [
        "#### **2.2.4 Verificación de respuestas (0.3 puntos)**\n",
        "\n",
        "Pruebe el funcionamiento de su agente y asegúrese que el agente esté ocupando correctamente las tools disponibles. ¿En qué casos el agente debería ocupar la tool de Tavily? ¿En qué casos debería ocupar la tool de Wikipedia?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pqo2dsxvywW_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZbDTYiogquv"
      },
      "source": [
        "### **2.3 Multi Agente (1.5 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media1.tenor.com/m/r7QMJLxU4BoAAAAd/this-is-getting-out-of-hand-star-wars.gif\"\n",
        "\" width=\"450\">\n",
        "</p>\n",
        "\n",
        "El objetivo de esta subsección es encapsular las funcionalidades creadas en una solución multiagente con un **supervisor**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-iUfH0WvI6m"
      },
      "source": [
        "#### **2.3.1 Generando Tools (0.5 puntos)**\n",
        "\n",
        "Transforme la solución RAG de la sección 2.1 y el agente de la sección 2.2 a *tools* (una tool por cada uno)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw1cfTtvv1AZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQYNjT_0vPCg"
      },
      "source": [
        "#### **2.3.2 Agente Supervisor (0.5 puntos)**\n",
        "\n",
        "Habilite un agente que tenga acceso a las tools del punto anterior y pueda responder preguntas relacionadas. Almacene este agente en una variable llamada supervisor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv2ZY0BAv1RD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea3zWlvyvY7K"
      },
      "source": [
        "#### **2.3.3 Verificación de respuestas (0.25 puntos)**\n",
        "\n",
        "Pruebe el funcionamiento de su agente repitiendo las preguntas realizadas en las secciones 2.1.4 y 2.2.4 y comente sus resultados. ¿Cómo varían las respuestas bajo este enfoque?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_1t0zkgv1qW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qb8bdAmYvgwn"
      },
      "source": [
        "#### **2.3.4 Análisis (0.25 puntos)**\n",
        "\n",
        "¿Qué diferencias tiene este enfoque con la solución *Router* vista en clases? Nombre al menos una ventaja y desventaja."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAUlJxqoLK5r"
      },
      "source": [
        "`escriba su respuesta acá`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JWVSuWiZ8Mj"
      },
      "source": [
        "### **2.4 Memoria (Bonus +0.5 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media1.tenor.com/m/Gs95aiElrscAAAAd/memory-unlocked-ratatouille-critic.gif\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "Una de las principales falencias de las soluciones que hemos visto hasta ahora es que nuestro chat no responde las interacciones anteriores, por ejemplo:\n",
        "\n",
        "- Pregunta 1: \"Hola! mi nombre es Sebastián\"\n",
        "  - Respuesta esperada: \"Hola Sebastián! ...\"\n",
        "- Pregunta 2: \"Cual es mi nombre?\"\n",
        "  - Respuesta actual: \"Lo siento pero no conozco tu nombre :(\"\n",
        "  - **Respuesta esperada: \"Tu nombre es Sebastián\"**\n",
        "\n",
        "Para solucionar esto, se les solicita agregar un componente de **memoria** a la solución entregada en el punto 2.3.\n",
        "\n",
        "**Nota: El Bonus es válido <u>sólo para la sección 2 de Large Language Models.</u>**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6Y7tIPJLPfB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFc3jBT5g0kT"
      },
      "source": [
        "### **2.5 Despliegue (0 puntos)**\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media1.tenor.com/m/IytHqOp52EsAAAAd/you-get-a-deploy-deploy.gif\"\n",
        "\" width=\"400\">\n",
        "</p>\n",
        "\n",
        "Una vez tengan los puntos anteriores finalizados, toca la etapa de dar a conocer lo que hicimos! Para eso, vamos a desplegar nuestro modelo a través de `gradio`, una librería especializada en el levantamiento rápido de demos basadas en ML.\n",
        "\n",
        "Primero instalamos la librería:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T8TsvnCPbkIA"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJBztEUovKsF"
      },
      "source": [
        "Luego sólo deben ejecutar el siguiente código e interactuar con la interfaz a través del notebook o del link generado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3KedQSvg1-n"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import time\n",
        "\n",
        "def agent_response(message, history):\n",
        "  '''\n",
        "  Función para gradio, recibe mensaje e historial, devuelte la respuesta del chatbot.\n",
        "  '''\n",
        "  # get chatbot response\n",
        "  response = ... # rellenar con la respuesta de su chat\n",
        "\n",
        "  # assert\n",
        "  assert type(response) == str, \"output de route_question debe ser string\"\n",
        "\n",
        "  # \"streaming\" response\n",
        "  for i in range(len(response)):\n",
        "    time.sleep(0.015)\n",
        "    yield response[: i+1]\n",
        "\n",
        "gr.ChatInterface(\n",
        "    agent_response,\n",
        "    type=\"messages\",\n",
        "    title=\"Chatbot MDS7202\", # Pueden cambiar esto si lo desean\n",
        "    description=\"Hola! Soy un chatbot muy útil :)\", # también la descripción\n",
        "    theme=\"soft\",\n",
        "    ).launch(\n",
        "        share=True, # pueden compartir el link a sus amig@s para que interactuen con su chat!\n",
        "        debug = False,\n",
        "        )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
